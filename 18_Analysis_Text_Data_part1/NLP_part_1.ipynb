{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ текстовых данных\n",
    "\n",
    "### Какие задачи можно решать, обрабатывая текст?\n",
    "\n",
    "1. синтаксические задачи\n",
    "  * разметка по частям речи и по морфологическим признакам\n",
    "  * деление слов в тексте на морфемы (суффикс, приставка и пр.)\n",
    "  * стемминг, лемматизация \n",
    "  * деление на предложения (инициалы и сокращения) и слова (китайский язык)\n",
    "  * поиск имен и названий в тексте - сущностей (named entity recognition)\n",
    "  * разрешение смысла слов в заданном контексте (замок/замок)\n",
    "  * построить синтаксическое дерево\n",
    "  * определение того, к каким другим объектам относится слово\n",
    "2. задачи на понимание текста, в которых есть \"учитель\"\n",
    "  * предсказание следующего символа\n",
    "  * информационный поиск\n",
    "  * анализ тональности\n",
    "  * выделение отношений и фактов\n",
    "  * ответы на вопросы\n",
    "3. понимание и порождение текста \n",
    "  * порождение текста\n",
    "  * машинный перевод\n",
    "  * диалоговые модели (чат-бот)\n",
    "  \n",
    "Косвенные задачи:\n",
    "  * описание изображения\n",
    "  * распознавание речи\n",
    "  \n",
    "**Задачи бизнеса**:\n",
    "  * распознавание речи (помощник)\n",
    "  * чат-бот (замена техподдержки в решении большинства вопросов)\n",
    "  * поиск точного ответа на вопрос в базе документов (например, база стандартов)\n",
    "  * оценка мнения в социальных сетях о продукте\n",
    "  * ... (ваши варианты?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()  # download lots of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# От текста к простым моделям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на токены\n",
    "**Def.**  \n",
    "разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n",
    "Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n",
    "\n",
    "\n",
    "*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n",
    "\n",
    "\n",
    "**Проблемы:**  \n",
    "* my.email@mail.ru, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "Альтернатива: n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Трисия\n",
      "любила\n",
      "Нью\n",
      "-\n",
      "Йорк\n",
      ",\n",
      "поскольку\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "s = u'Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.'\n",
    "\n",
    "for t in tokenizer.tokenize(s)[:7]: \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ftfy: fixes text for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm blue, da ba dee da ba doo...\n"
     ]
    }
   ],
   "source": [
    "from ftfy import fix_text\n",
    "print(fix_text(u'\\001\\033[36;44mI&#x92;m blue, da ba dee da ba doo&#133;\\033[0m', normalization='NFKC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова\n",
    "\n",
    "Наиболее частые слова в языке, не содержащие никакой информации о содержании текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и в во не что он на я с со как а то все она так его но да ты\n",
      "i me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print (' '.join(stopwords.words('russian')[:20]))\n",
    "print (' '.join(stopwords.words('english')[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация\n",
    "\n",
    "Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n",
    "\n",
    "Подходы  \n",
    "* сформулировать набор правил, по которым преобразуется токен  \n",
    "Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n",
    "* явно хранить связи между токенами (WordNet – Princeton)  \n",
    "машина → автомобиль, Windows 6→ window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью-йорк\n"
     ]
    }
   ],
   "source": [
    "s = 'Нью-Йорк'\n",
    "s1 = s.lower()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюйорк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s2 = re.sub(\"\\W\", \"\", s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюиорк\n"
     ]
    }
   ],
   "source": [
    "s3 = re.sub(\"й\", u\"и\", s2)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг и Лемматизация\n",
    "\n",
    "Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n",
    "\n",
    "* Stemming – с помощью простых эвристических правил\n",
    "  * Porter (Cambridge – 1980)\n",
    "        5 этапов, на каждом применяется набор правил, таких как\n",
    "            sses → ss (caresses → caress)\n",
    "            ies → i (ponies → poni)\n",
    "\n",
    "  * Lovins (1968)\n",
    "  * Paice (1990)\n",
    "  * другие\n",
    "* Lemmatization – с использованием словарей и морфологического анализа\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "stem\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "s = PorterStemmer()\n",
    "print (s.stem('Tokenization'))\n",
    "print (s.stem('stemming'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "авиац\n",
      "национальн\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "r = RussianStemmer()\n",
    "print(r.stem('Авиация'))\n",
    "print(r.stem('национальный'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация \n",
    "(обычно лучше работает для сложных языков, в том числе для русского)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Parse(word='замок', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='замок', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 139, 0),))\n",
      "Word: замок | Normal form: замок\n",
      "\n",
      "\n",
      "Metadata: Parse(word='замок', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='замок', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 139, 3),))\n",
      "Word: замок | Normal form: замок\n",
      "\n",
      "\n",
      "Metadata: Parse(word='замок', tag=OpencorporaTag('VERB,perf,intr masc,sing,past,indc'), normal_form='замокнуть', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 730, 1),))\n",
      "Word: замок | Normal form: замокнуть\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for i in morph.parse(u'замок'):\n",
    "    print(\"Metadata: {}\".format(i))\n",
    "    print(\"Word: {} | Normal form: {}\".format(i.word, i.normal_form))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Представление документов\n",
    "\n",
    "**Boolean Model.** Присутствие или отсутствие слова в документе  \n",
    "**Bag of Words.** Порядок токенов не важен  \n",
    "\n",
    "*Погода была ужасная, принцесса была прекрасная.\n",
    "Или все было наоборот?*\n",
    "\n",
    "Координаты\n",
    "* Мультиномиальные: количество токенов в документе\n",
    "* Числовые: взвешенное количество токенов в документе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 1.],\n",
       "       [0., 1., 3.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer = DictVectorizer(sparse=False)\n",
    "text_dict = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = dvectorizer.fit_transform(text_dict)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.transform({'foo': 4, 'unseen_feature': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'thank': 1, '40': 1, ',': 1, 'mr': 1, 'president': 1, '.': 1}),\n",
       " Counter({'madam': 1,\n",
       "          'president': 1,\n",
       "          ',': 3,\n",
       "          'agree': 1,\n",
       "          'recognise': 1,\n",
       "          'turkey': 2,\n",
       "          \"'\": 1,\n",
       "          'european': 1,\n",
       "          'prospects': 2,\n",
       "          'auspicious': 1,\n",
       "          'outcome': 1,\n",
       "          'needs': 1,\n",
       "          ':': 1}),\n",
       " Counter({'madam': 1,\n",
       "          'president': 1,\n",
       "          ',': 2,\n",
       "          'firstly': 1,\n",
       "          'would': 1,\n",
       "          'like': 1,\n",
       "          'express': 1,\n",
       "          'sincerest': 1,\n",
       "          'thanks': 1,\n",
       "          'high': 1,\n",
       "          'representative': 1,\n",
       "          'including': 1,\n",
       "          'important': 1,\n",
       "          'issue': 1,\n",
       "          'agenda': 1,\n",
       "          'early': 1,\n",
       "          'stage': 1,\n",
       "          '.': 1})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "docs = [\n",
    "    \"Thank 40 you, Mr President.\",\n",
    "    \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
    "    \"Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.\",\n",
    "]\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "stopwords_eng = stopwords.words()\n",
    "\n",
    "document_bags = list()\n",
    "\n",
    "for d in docs:\n",
    "    bag = Counter()\n",
    "    text = d.lower()\n",
    "    for t in tokenizer.tokenize(text):     \n",
    "        if t in stopwords_eng:\n",
    "            continue\n",
    "        bag[t] += 1\n",
    "    document_bags.append(bag)\n",
    "    \n",
    "document_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 3., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 2., 1., 0., 0., 0., 0., 0., 2., 0.],\n",
       "       [0., 2., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.fit_transform(document_bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " ',',\n",
       " '.',\n",
       " '40',\n",
       " ':',\n",
       " 'agenda',\n",
       " 'agree',\n",
       " 'auspicious',\n",
       " 'early',\n",
       " 'european',\n",
       " 'express',\n",
       " 'firstly',\n",
       " 'high',\n",
       " 'important',\n",
       " 'including',\n",
       " 'issue',\n",
       " 'like',\n",
       " 'madam',\n",
       " 'mr',\n",
       " 'needs',\n",
       " 'outcome',\n",
       " 'president',\n",
       " 'prospects',\n",
       " 'recognise',\n",
       " 'representative',\n",
       " 'sincerest',\n",
       " 'stage',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'turkey',\n",
       " 'would']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 2, 1, 0,\n",
       "         0, 0, 0, 0, 2],\n",
       "        [0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sklearn_vectorizer = CountVectorizer(stop_words='english')\n",
    "sklearn_vectorizer.fit_transform(docs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 23,\n",
       " '40': 0,\n",
       " 'mr': 14,\n",
       " 'president': 17,\n",
       " 'madam': 13,\n",
       " 'agree': 2,\n",
       " 'recognise': 19,\n",
       " 'turkey': 25,\n",
       " 'european': 5,\n",
       " 'prospects': 18,\n",
       " 'auspicious': 3,\n",
       " 'outcome': 16,\n",
       " 'needs': 15,\n",
       " 'firstly': 7,\n",
       " 'like': 12,\n",
       " 'express': 6,\n",
       " 'sincerest': 21,\n",
       " 'thanks': 24,\n",
       " 'high': 8,\n",
       " 'representative': 20,\n",
       " 'including': 10,\n",
       " 'important': 9,\n",
       " 'issue': 11,\n",
       " 'agenda': 1,\n",
       " 'early': 4,\n",
       " 'stage': 22}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Количество вхождений слова $t$ в документе $d$\n",
    "$$\n",
    "TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d)\n",
    "$$\n",
    "Количество документов из $N$ возможных, где встречается $t$\n",
    "$$\n",
    "DF_t = document\\!\\!-\\!\\!fequency(t)\n",
    "$$\n",
    "$$\n",
    "IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t}\n",
    "$$\n",
    "TF-IDF\n",
    "$$\n",
    "TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "$$\n",
    "\n",
    "Оценивает важность слова в контексте документа, являющегося частью корпуса\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.54645401, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.54645401,\n",
       "         0.        , 0.        , 0.32274454, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.54645401, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.25882751, 0.25882751, 0.        ,\n",
       "         0.25882751, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.19684499, 0.        ,\n",
       "         0.25882751, 0.25882751, 0.1528677 , 0.51765502, 0.25882751,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.51765502],\n",
       "        [0.        , 0.26795858, 0.        , 0.        , 0.26795858,\n",
       "         0.        , 0.26795858, 0.26795858, 0.26795858, 0.26795858,\n",
       "         0.26795858, 0.26795858, 0.26795858, 0.20378941, 0.        ,\n",
       "         0.        , 0.        , 0.15826066, 0.        , 0.        ,\n",
       "         0.26795858, 0.26795858, 0.26795858, 0.        , 0.26795858,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "features = vectorizer.fit_transform(docs).todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 23,\n",
       " '40': 0,\n",
       " 'mr': 14,\n",
       " 'president': 17,\n",
       " 'madam': 13,\n",
       " 'agree': 2,\n",
       " 'recognise': 19,\n",
       " 'turkey': 25,\n",
       " 'european': 5,\n",
       " 'prospects': 18,\n",
       " 'auspicious': 3,\n",
       " 'outcome': 16,\n",
       " 'needs': 15,\n",
       " 'firstly': 7,\n",
       " 'like': 12,\n",
       " 'express': 6,\n",
       " 'sincerest': 21,\n",
       " 'thanks': 24,\n",
       " 'high': 8,\n",
       " 'representative': 20,\n",
       " 'including': 10,\n",
       " 'important': 9,\n",
       " 'issue': 11,\n",
       " 'agenda': 1,\n",
       " 'early': 4,\n",
       " 'stage': 22}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
